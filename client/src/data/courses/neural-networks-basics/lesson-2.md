# 順伝播と逆伝播 - 学習のメカニズム

## このレッスンで学ぶこと

このレッスンを完了すると、ニューラルネットワークがどのように学習するかを理解し、順伝播と逆伝播の仕組みを直感的に理解できるようになります。
医師が経験を積んで診断の精度を上げるプロセスと、AIの学習プロセスを比較しながら学びます。

---

## セクション1: 順伝播 - 情報を前へ進める

### 順伝播とは：入力から出力へ

順伝播（Forward Propagation）は、入力データをネットワークを通して前へ進め、最終的な出力を得るプロセスです。

**順伝播の流れ：**
1. 入力層：データを受け取る（例：患者の症状、検査結果）
2. 中間層：情報を処理する（重要な特徴を抽出）
3. 出力層：結果を出力する（例：診断結果）

**医療での例：**
医師が診断する際も、同様の流れで判断します。
- 患者の情報を受け取る（入力）
- 重要な所見を抽出し、総合的に判断する（処理）
- 診断を下す（出力）

### 活性化関数：出力を決定する

活性化関数（Activation Function）は、ノードの出力を決定する関数です。
入力の合計を、特定の範囲に変換します。

**主な活性化関数：**
- **シグモイド関数**：0から1の範囲に変換（確率など）
- **ReLU関数**：負の値を0にし、正の値はそのまま（画像処理など）

**医療での例：**
診断の際、医師は「この疾患の可能性が高い（0.8）」「この疾患の可能性が低い（0.2）」というような確率を考えます。
活性化関数は、AIの判断をこのような形に変換します。

### 予測と実際の比較：学習の出発点

順伝播によって得られた予測と、実際の結果（正解）を比較することが、学習の出発点です。

**医療での例：**
研修医が先輩医師の診断を見て、「自分の判断は合っていたか」「どこが間違っていたか」を確認するのと同様です。

---

## セクション2: 逆伝播 - 間違いから学ぶ

### 逆伝播とは：後ろから前へ

逆伝播（Backpropagation）は、予測と実際の結果の差（誤差）を後ろから前へ伝え、重みとバイアスを調整するプロセスです。

**逆伝播の流れ：**
1. 出力層：予測と実際の結果の差を計算（誤差）
2. 中間層：誤差を前の層に伝える（どの部分が間違っていたか）
3. 入力層：すべての層で重みとバイアスを調整

**医療での例：**
医師が診断を間違えた場合、「なぜ間違えたか」を振り返り、「次回はどの情報を重視すべきか」を学びます。
逆伝播も、同様に「どこが間違っていたか」を分析し、「次回はどの重みを調整すべきか」を決めます。

### 誤差の計算：どれだけ間違えたか

誤差（Error）は、予測と実際の結果の差です。

**医療での例：**
「この疾患の可能性は80%」と予測したが、実際は別の疾患だった場合、誤差が大きいことになります。
誤差が大きいほど、大きく調整する必要があります。

### 勾配降下法：誤差を減らす方向へ

勾配降下法（Gradient Descent）は、誤差を減らす方向に重みとバイアスを調整する方法です。

**医療での例：**
医師が経験を積むことで、診断の精度が上がっていくのと同様です。
AIも、学習を重ねることで、予測の精度が上がっていきます。

---

## セクション3: 学習のプロセス

### エポック：1回の学習サイクル

エポック（Epoch）は、すべての学習データを1回学習することを指します。

**医療での例：**
研修医が、同じような症例を何度も見て学ぶのと同様です。
通常、複数のエポックで学習を行い、性能を向上させます。

### バッチ：一度に処理するデータ数

バッチ（Batch）は、一度に処理する学習データの数です。

**バッチサイズの影響：**
- **バッチサイズが大きい**：一度に多くのデータを処理（安定した学習）
- **バッチサイズが小さい**：細かく調整しながら学習（柔軟な学習）

**医療での例：**
医師が、一度に多くの症例を処理するか、1つずつ慎重に処理するかの違いと似ています。

### 学習率：学習の速度

学習率（Learning Rate）は、学習の進む速さを調整するパラメータです。

**学習率の影響：**
- **学習率が高い**：早く学習するが、不安定になる可能性
- **学習率が低い**：安定して学習するが、時間がかかる

**医療での例：**
医師が、経験を素早く吸収するか、時間をかけてじっくり学ぶかの違いと似ています。
医療AIでは、高い精度が求められるため、適切な学習率の設定が重要です。

---

## セクション4: 損失関数：学習の目標

### 損失関数とは：誤差を測る尺度

損失関数（Loss Function）は、予測と実際の結果の差を数値で表す関数です。
この値を最小化することが、学習の目標です。

**主な損失関数：**
- **平均二乗誤差（MSE）**：予測値と実際の値の差の二乗の平均（回帰問題）
- **交差エントロピー誤差（Cross-entropy）**：予測した確率分布と実際の確率分布の差（分類問題）

**医療での例：**
診断支援システムでは、分類問題が多いため、交差エントロピー誤差がよく使われます。
「この疾患である可能性が80%」という予測が、「実際には別の疾患だった」場合、誤差が大きくなります。

### 最適化：誤差を最小化する

最適化（Optimization）は、損失関数の値を最小化するプロセスです。

**医療での例：**
医師が経験を積むことで、診断の精度が上がり、誤診が減っていくのと同様です。
AIも、学習を重ねることで、損失関数の値が小さくなり、予測の精度が上がります。

---

## まとめ

このレッスンでは、ニューラルネットワークの学習メカニズム（順伝播と逆伝播）を学びました。

次回のレッスンでは、活性化関数の詳細と、なぜ非線形の問題を解決できるのかについて詳しく学びます。
医療現場での複雑な判断プロセスを理解する上で、重要な概念です。

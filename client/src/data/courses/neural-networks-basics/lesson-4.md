# 損失関数と最適化手法

## このレッスンで学ぶこと

このレッスンを完了すると、損失関数と最適化手法について理解し、ニューラルネットワークがどのように学習の目標を達成するかを理解できるようになります。
医療現場での学習プロセスと比較しながら学びます。

---

## セクション1: 損失関数の基礎

### 損失関数とは：学習の目標

損失関数（Loss Function）は、予測と実際の結果の差を数値で表す関数です。
この値を最小化することが、学習の目標です。

**医療での例：**
医師が経験を積むことで、診断の精度が上がり、誤診が減っていくのと同様です。
AIも、学習を重ねることで、損失関数の値が小さくなり、予測の精度が上がります。

### 主な損失関数

**平均二乗誤差（MSE）：**
- 回帰問題で使われる
- 予測値と実際の値の差の二乗の平均

**交差エントロピー誤差：**
- 分類問題で使われる
- 予測した確率分布と実際の確率分布の差

**医療での重要性：**
診断支援システムでは、分類問題が多いため、交差エントロピー誤差がよく使われます。

---

## セクション2: 最適化手法

### 勾配降下法：誤差を減らす方向へ

勾配降下法（Gradient Descent）は、損失関数の値を最小化する方法です。
誤差を減らす方向に重みとバイアスを調整します。

**医療での例：**
医師が経験を積むことで、診断の精度が上がっていくのと同様です。

### 確率的勾配降下法（SGD）：効率的な学習

確率的勾配降下法（Stochastic Gradient Descent：SGD）は、データをランダムに選んで学習する方法です。

**利点：**
- 計算が効率的
- 局所最適解に陥りにくい

### Adam：現在最もよく使われる

Adamは、現在最もよく使われる最適化手法です。
学習率を自動的に調整します。

**利点：**
- 学習率の調整が自動
- 収束が速い
- 多くの問題で良好な性能

---

## セクション3: 過学習と対策

### 過学習とは：データに依存しすぎる

過学習（Overfitting）は、学習データに過剰に適合し、新しいデータに対する予測が悪くなる現象です。

**医療での問題：**
- 特定の病院のデータで学習したモデルが、他の病院のデータでは性能が落ちる

### 対策：正則化

正則化（Regularization）は、過学習を防ぐための技術です。

**主な手法：**
- **ドロップアウト**：一部のノードをランダムに無効化する
- **L2正則化**：重みの値を小さくする

---

## まとめ

このレッスンでは、損失関数と最適化手法について学びました。

次回のレッスンでは、深層学習と畳み込みニューラルネットワーク（CNN）について詳しく学びます。
医療画像解析で広く使われている技術です。

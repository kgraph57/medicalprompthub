# 活性化関数と非線形性

## このレッスンで学ぶこと

このレッスンを完了すると、活性化関数の重要性と、なぜニューラルネットワークが複雑な問題を解決できるのかを理解できるようになります。
医療現場での複雑な判断プロセスを理解する上で、重要な概念です。

---

## セクション1: 活性化関数とは

### 活性化関数の役割：出力を変換する

活性化関数（Activation Function）は、ノードの出力を決定する関数です。
入力の合計を、特定の範囲や形式に変換します。

**活性化関数がない場合：**
- ニューラルネットワークは、線形変換の組み合わせにしかならない
- 複雑な非線形な問題を解決できない

**医療での例：**
診断の際、医師は単純な足し算ではなく、複雑な判断を行います。
「この症状がある」「この検査結果がある」という単純な組み合わせではなく、複数の要因が複雑に絡み合って診断を下します。

### 非線形性：複雑な問題に対応

非線形性（Non-linearity）は、直線では表現できない複雑な関係です。

**医療での重要性：**
- 症状と診断の関係は、単純な線形関係ではない
- 複数の要因が複雑に絡み合っている

活性化関数により、ニューラルネットワークは非線形な問題を解決できるようになります。

---

## セクション2: 主な活性化関数

### シグモイド関数：確率を表現

シグモイド関数（Sigmoid Function）は、出力を0から1の範囲に変換します。

**医療での活用：**
- 疾患の可能性を確率で表現（0.8 = 80%の可能性）
- 二値分類問題（正常/異常など）

**特徴：**
- 出力が0から1の範囲
- 滑らかな曲線
- 勾配消失問題が起こりやすい

### ReLU関数：現在最もよく使われる

ReLU（Rectified Linear Unit）関数は、負の値を0にし、正の値はそのまま出力します。

**医療での活用：**
- 画像診断支援（画像処理でよく使われる）
- 深層学習で広く使われている

**特徴：**
- 計算が簡単で高速
- 勾配消失問題が起こりにくい
- 負の値が失われる可能性がある

### ソフトマックス関数：多クラス分類

ソフトマックス関数（Softmax Function）は、複数の出力の合計を1にし、各出力を確率として扱います。

**医療での活用：**
- 複数の診断候補の可能性を確率で表現
- 多クラス分類問題

**例：**
- 疾患Aの可能性：0.6（60%）
- 疾患Bの可能性：0.3（30%）
- 疾患Cの可能性：0.1（10%）
- 合計：1.0（100%）

---

## セクション3: 活性化関数の選択

### 問題に応じた選択

活性化関数の選択は、解決したい問題に応じて決まります。

**選択の基準：**
- **二値分類**：シグモイド関数
- **多クラス分類**：ソフトマックス関数
- **画像処理**：ReLU関数
- **回帰問題**：線形関数（活性化関数なし）

### 深層学習での注意点

深層学習では、勾配消失問題に注意が必要です。

**勾配消失問題：**
- 多くの層を持つネットワークで、勾配が小さくなりすぎる問題
- 学習が進まなくなる

**対策：**
- ReLU関数を使う（勾配消失問題が起こりにくい）
- バッチ正規化などの技術を使う

---

## まとめ

このレッスンでは、活性化関数の重要性と、主な活性化関数の特徴を学びました。

次回のレッスンでは、損失関数と最適化手法について詳しく学びます。
ニューラルネットワークの学習を理解する上で、重要な概念です。

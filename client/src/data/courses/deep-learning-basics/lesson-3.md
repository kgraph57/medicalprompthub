# 活性化関数とその役割

## 🎯 このレッスンで学ぶこと

このレッスンを完了すると、シグモイド、ReLU、tanhなどの活性化関数、なぜ非線形関数が必要か、各関数の特徴を理解できるようになります。

---

## 📖 セクション1: 活性化関数とは

### 活性化関数の役割

活性化関数は、ニューロンの出力を決定する関数です。

**活性化関数の役割**：
- **非線形性**：非線形な変換を提供
- **出力の範囲**：出力の範囲を制限
- **学習の効率**：学習の効率に影響

### なぜ非線形関数が必要か

非線形関数がないと、多層ネットワークも単層ネットワークと同じ能力しか持てません。

**非線形関数の重要性**：
- **表現力**：非線形な関係を表現
- **複雑なパターン**：複雑なパターンを学習可能

---

## 📖 セクション2: 主要な活性化関数

### シグモイド関数

シグモイド関数は、出力を0から1の範囲に制限します。

**シグモイド関数の特徴**：
- **出力範囲**：0から1
- **滑らか**：滑らかな関数
- **勾配消失**：勾配消失の問題がある

### ReLU関数

ReLU（Rectified Linear Unit）は、現在最もよく使われる活性化関数です。

**ReLU関数の特徴**：
- **計算効率**：計算が高速
- **勾配消失の回避**：勾配消失を回避
- **スパース性**：スパースな活性化

### tanh関数

tanh関数は、出力を-1から1の範囲に制限します。

**tanh関数の特徴**：
- **出力範囲**：-1から1
- **中心化**：出力が中心化される
- **勾配消失**：勾配消失の問題がある

---

## 📖 セクション3: 活性化関数の選択

### 用途別の選択

活性化関数は、用途に応じて選択します。

**選択の指針**：
- **隠れ層**：ReLUが一般的
- **出力層（分類）**：シグモイド、ソフトマックス
- **出力層（回帰）**：線形関数

### 医療現場での応用

**医療現場での例**：
- **診断（分類）**：出力層にシグモイドやソフトマックス
- **予測（回帰）**：出力層に線形関数
- **隠れ層**：ReLUが一般的

---

## 💡 重要な洞察：活性化関数の重要性

活性化関数は、ニューラルネットワークの性能に大きく影響します。

**実践的なアドバイス**：
- **ReLU**：隠れ層で一般的
- **シグモイド/ソフトマックス**：分類の出力層
- **線形関数**：回帰の出力層

---

## 🎓 まとめ：活性化関数とその役割を理解する

このレッスンでは、活性化関数とその役割について学びました。

**重要なポイント**：

1. **活性化関数の役割**：非線形性の提供、出力範囲の制限
2. **主要な活性化関数**：シグモイド、ReLU、tanh
3. **活性化関数の選択**：用途に応じた選択

### 次のステップ

次のレッスンでは、損失関数と最適化について学びます。平均二乗誤差、交差エントロピー損失、勾配降下法を理解します。

---

**更新日**: 2025年12月
